{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape college applications\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Pre-processing\n",
    "2. Counting word occurances\n",
    "3. Making individual dataframes\n",
    "4. Merging those dataframes\n",
    "\n",
    "Inspired by: https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from operator import itemgetter\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def stem_words(words):\n",
    "    # Stem words in list of tokenized words\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    # Lemmatize verbs in list of tokenized words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def remove_pmarks(text):\n",
    "    text = re.sub(r'~|`|:|;|\"|,', '', text)\n",
    "    text = str.replace(text, '\"', '')\n",
    "    text = str.replace(text, \"'\", '')\n",
    "    text = str.replace(text, '.', '')\n",
    "    text = str.replace(text, '?', '')\n",
    "    return text\n",
    "\n",
    "def remove_common_words(text):\n",
    "    words_to_drop = ['a','is','it','of','in','at','to','the']\n",
    "    for word in words_to_drop:\n",
    "        text = re.sub(''.join((r'\\b', word, r'\\b')), '', text)\n",
    "    return text\n",
    "\n",
    "def combine_cword(text):\n",
    "    compound_words = {\n",
    "        'social security number': 'socialsecuritynumber',\n",
    "        'ssn': 'socialsecuritynumber',\n",
    "        'high school': 'highschool'\n",
    "    }\n",
    "    for cword in compound_words:\n",
    "        text = re.sub(''.join((r'\\b', cword, r'\\b')), compound_words[cword], text)\n",
    "    return text\n",
    "\n",
    "def get_words(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    # Remove non-ASCII characters from list of tokenized words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    # Replace all interger occurrences in list of tokenized words with textual representation\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(text):\n",
    "    # Remove punctuation from entire string.\n",
    "    print('Remove punctuation.', end='')\n",
    "    text = remove_pmarks(text)\n",
    "    print(' DONE')\n",
    "    # Put to lowercase first\n",
    "    print('Converting case....', end='')\n",
    "    text = text.lower()\n",
    "    print(' DONE')\n",
    "    # Remove unwanted common words.\n",
    "    print('Remove freq words..', end='')\n",
    "    text = remove_common_words(text)\n",
    "    print(' DONE')\n",
    "    # Handle common application compound words\n",
    "    print('Handle compounds...', end='')\n",
    "    text = combine_cword(text)\n",
    "    print(' DONE')\n",
    "    # Tokenize the string.\n",
    "    print('Tokenize string....', end='')\n",
    "    words = get_words(text)\n",
    "    print(' DONE')\n",
    "    # Remove ascii characters.\n",
    "    print('Remove non ascii...', end='')\n",
    "    words = remove_non_ascii(words)\n",
    "    print(' DONE')\n",
    "    # Convert numebrs to words.\n",
    "    print('Replace numbers....', end='')\n",
    "    words = replace_numbers(words)\n",
    "    print(' DONE')\n",
    "    return words\n",
    "\n",
    "def get_sorted_count(words):\n",
    "    index_list = sorted(set(words))\n",
    "    count_list = list(range(len(index_list)))\n",
    "    for i in range(len(index_list)):\n",
    "        count_list[i] = 0\n",
    "        for word in words:\n",
    "            if word == index_list[i]:\n",
    "                count_list[i] = count_list[i] + 1\n",
    "    grand_list = []\n",
    "    for i in range(len(index_list)):\n",
    "        item_list = []\n",
    "        item_list.append(index_list[i])\n",
    "        item_list.append(count_list[i])\n",
    "        grand_list.append(item_list)\n",
    "    return sorted(grand_list, key=itemgetter(0), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_test_text = '''Here are a few words that I will use to test.\n",
    "high\n",
    "Don't think this is the end of it.\n",
    "high school\n",
    "Social Security Number\n",
    "ssn 'Here are a few words that I will use to test.\n",
    "high\n",
    "Don't think this is the end of it.\n",
    "high school\n",
    "SSN\n",
    "high school\n",
    "social security number\n",
    "1. Hi hi hi\n",
    "2. Three four five\n",
    "6. What?'''\n",
    "\n",
    "more_test_text = '''Cookie information can be found through the use of a\n",
    "high school SSN high school plug-in for your web browser. (I use \n",
    "'Cookie Manager' on FireFox, although there are many other options \n",
    "for FireFox and other browsers). The two cookies you are looking \n",
    "for are called Y and T, and they are linked to the domain yahoo.com.\n",
    "Extract the data from these cookies, and paste it into the appropriate \n",
    "variables... a cookie will expire after a certain amount of time, \n",
    "which varies between computers. This means that you may have to \n",
    "re-fetch the Y and T cookie data every few days, or you will not be\n",
    "able to archive private groups. 'Here are a few words that I \n",
    "will use to test. high Don't think this is the end of it. high school'''\n",
    "\n",
    "print(sorted(set(normalize(some_test_text))))\n",
    "print(sorted(set(normalize(more_test_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vname in ['some_test_text', 'more_test_text']:\n",
    "    vars()[vname + '_df'] = DataFrame(get_sorted_count(normalize(vars()[vname])), columns=['word','freq'])\n",
    "    \n",
    "some_test_text_df['freq1'] = some_test_text_df['freq']\n",
    "some_test_text_df = some_test_text_df.drop(columns=['freq'])\n",
    "print(some_test_text_df.head())\n",
    "more_test_text_df['freq2'] = more_test_text_df['freq']\n",
    "more_test_text_df = more_test_text_df.drop(columns=['freq'])\n",
    "print(more_test_text_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(some_test_text_df,more_test_text_df,on='word',how='outer').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
