{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape college applications\n",
    "\n",
    "This notebook was developed to work with pdfs scraped with `ColAppScrape.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def get_words(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    # Remove non-ASCII characters from list of tokenized words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    # Convert all characters to lowercase from list of tokenized words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_contractions(text):\n",
    "    # Replace contractions in string of text\n",
    "    return contractions.fix(text, leftovers=False)\n",
    "\n",
    "def remove_contractions(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = replace_contractions(word)\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    # Remove punctuation from list of tokenized words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    # Replace all interger occurrences in list of tokenized words with textual representation\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    # Remove stop words from list of tokenized words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    # Stem words in list of tokenized words\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    # Lemmatize verbs in list of tokenized words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def remove_pmarks(text):\n",
    "    text = re.sub(r'~|`|:|;|\"|,', '', text)\n",
    "    text = str.replace(text, '\"', '')\n",
    "    text = str.replace(text, \"'\", '')\n",
    "    text = str.replace(text, '.', '')\n",
    "    text = str.replace(text, '?', '')\n",
    "    return text\n",
    "\n",
    "def handl_ssn(text):\n",
    "    text = str.replace(text, 'social security number', 'socialsecuritynumber')\n",
    "    text = str.replace(text, 'ssn', 'socialsecuritynumber')\n",
    "    return text\n",
    "\n",
    "def normalize(text):\n",
    "    # Remove punctuation from entire string.\n",
    "    print('Remove punctuation.', end='')\n",
    "    text = remove_pmarks(text)\n",
    "    print(' DONE')\n",
    "    \n",
    "    # Put to lowercase first\n",
    "    print('Converting case....', end='')\n",
    "    text = text.lower()\n",
    "    print(' DONE')\n",
    "    \n",
    "    # Handle social security number\n",
    "    print('Handle ss numbers..', end='')\n",
    "    text = handl_ssn(text)\n",
    "    print(' DONE')\n",
    "    \n",
    "    # Tokenize the string.\n",
    "    print('Tokenize string....', end='')\n",
    "    words = get_words(text)\n",
    "    print(' DONE')\n",
    "    \n",
    "    # Remove ascii characters.\n",
    "    print('Remove non ascii...', end='')\n",
    "    words = remove_non_ascii(words)\n",
    "    print(' DONE')\n",
    "    \n",
    "    # Put everything to lowercase.\n",
    "    # print('To lowercase txt...', end='')\n",
    "    # words = to_lowercase(words)\n",
    "    # print(' DONE')\n",
    "    \n",
    "    # For this use case contractions managed above.\n",
    "    # print('Remove contracts...', end='')\n",
    "    # words = remove_contractions(words)\n",
    "    # print(' DONE')\n",
    "    \n",
    "    # For this use case punctuation managed above.\n",
    "    # print('Remove puncuation..', end='')\n",
    "    # words = remove_punctuation(words)\n",
    "    # print(' DONE')\n",
    "    \n",
    "    # Convert numebrs to words.\n",
    "    print('Replace numbers....', end='')\n",
    "    words = replace_numbers(words)\n",
    "    print(' DONE')\n",
    "    \n",
    "    # For this use case, leaving stop words.\n",
    "    # print('Remove stopwords...', end='')\n",
    "    # words = remove_stopwords(words)\n",
    "    # print(' DONE')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove punctuation. DONE\n",
      "Converting case.... DONE\n",
      "Handle ss numbers.. DONE\n",
      "here are a few words that i will use to test\n",
      "\n",
      "dont think this is the end of it\n",
      "\n",
      "socialsecuritynumber\n",
      "\n",
      "socialsecuritynumber\n",
      "\n",
      "socialsecuritynumber\n",
      "\n",
      "socialsecuritynumber\n",
      "\n",
      "1 hi hi hi\n",
      "2 three four five\n",
      "6 what\n",
      "Tokenize string.... DONE\n",
      "Remove non ascii... DONE\n",
      "Replace numbers.... DONE\n",
      "['here', 'are', 'a', 'few', 'words', 'that', 'i', 'will', 'use', 'to', 'test', 'dont', 'think', 'this', 'is', 'the', 'end', 'of', 'it', 'socialsecuritynumber', 'socialsecuritynumber', 'socialsecuritynumber', 'socialsecuritynumber', 'one', 'hi', 'hi', 'hi', 'two', 'three', 'four', 'five', 'six', 'what']\n"
     ]
    }
   ],
   "source": [
    "some_test_text = '''Here are a few words that I will use to test.\n",
    "\n",
    "Don't think this is the end of it.\n",
    "\n",
    "Social Security Number\n",
    "\n",
    "ssn\n",
    "\n",
    "SSN\n",
    "\n",
    "social security number\n",
    "\n",
    "1. Hi hi hi\n",
    "2. Three four five\n",
    "6. What?'''\n",
    "\n",
    "print(normalize(some_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
