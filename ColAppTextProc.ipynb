{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape college applications\n",
    "\n",
    "This notebook was developed to work with pdfs scraped with `ColAppScrape.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 81,
=======
   "execution_count": 70,
>>>>>>> ee4d413fa3caa8236139f74f5a69641777b5b52d
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from operator import itemgetter\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 82,
=======
   "execution_count": 76,
>>>>>>> ee4d413fa3caa8236139f74f5a69641777b5b52d
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def stem_words(words):\n",
    "    # Stem words in list of tokenized words\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    # Lemmatize verbs in list of tokenized words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def remove_pmarks(text):\n",
    "    text = re.sub(r'~|`|:|;|\"|,', '', text)\n",
    "    text = str.replace(text, '\"', '')\n",
    "    text = str.replace(text, \"'\", '')\n",
    "    text = str.replace(text, '.', '')\n",
    "    text = str.replace(text, '?', '')\n",
    "    return text\n",
    "\n",
    "def remove_common_words(text):\n",
    "    words_to_drop = ['a','is','it','of','in','at','to','the']\n",
    "    for word in words_to_drop:\n",
    "        text = re.sub(''.join((r'\\b', word, r'\\b')), '', text)\n",
    "    return text\n",
    "\n",
    "def handl_compounds(text):\n",
    "    compound_words = {\n",
    "        'social security number': 'socialsecuritynumber',\n",
    "        'ssn': 'socialsecuritynumber',\n",
    "        'high school': 'highschool'\n",
    "    }\n",
    "    for compound in compound_words:\n",
    "        text = re.sub(''.join((r'\\b', compound, r'\\b')), compound_words[compound], text)\n",
    "    # for compound in compound_words:\n",
    "    #     text = re.sub(''.join((r'\\b', compound, r'\\b')), '', text)\n",
    "    # text = str.replace(text, 'social security number', 'socialsecuritynumber')\n",
    "    # text = str.replace(text, 'ssn', 'socialsecuritynumber')\n",
    "    return text\n",
    "\n",
    "def get_words(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    # Remove non-ASCII characters from list of tokenized words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    # Replace all interger occurrences in list of tokenized words with textual representation\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(text):\n",
    "    # Remove punctuation from entire string.\n",
    "    print('Remove punctuation.', end='')\n",
    "    text = remove_pmarks(text)\n",
    "    print(' DONE')\n",
    "    # Put to lowercase first\n",
    "    print('Converting case....', end='')\n",
    "    text = text.lower()\n",
    "    print(' DONE')\n",
    "    # Remove unwanted common words.\n",
    "    print('Remove freq words..', end='')\n",
    "    text = remove_common_words(text)\n",
    "    print(' DONE')\n",
    "    # Handle common application compound words\n",
<<<<<<< HEAD
    "    print('Handle compounds...', end='')\n",
=======
    "    print('Handle compounds..', end='')\n",
>>>>>>> ee4d413fa3caa8236139f74f5a69641777b5b52d
    "    text = handl_compounds(text)\n",
    "    print(' DONE')\n",
    "    # Tokenize the string.\n",
    "    print('Tokenize string....', end='')\n",
    "    words = get_words(text)\n",
    "    print(' DONE')\n",
    "    # Remove ascii characters.\n",
    "    print('Remove non ascii...', end='')\n",
    "    words = remove_non_ascii(words)\n",
    "    print(' DONE')\n",
    "    # Convert numebrs to words.\n",
    "    print('Replace numbers....', end='')\n",
    "    words = replace_numbers(words)\n",
    "    print(' DONE')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 83,
=======
   "execution_count": 77,
>>>>>>> ee4d413fa3caa8236139f74f5a69641777b5b52d
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove punctuation. DONE\n",
      "Converting case.... DONE\n",
      "Remove freq words.. DONE\n",
<<<<<<< HEAD
      "Handle compounds... DONE\n",
=======
      "Handle compounds.. DONE\n",
>>>>>>> ee4d413fa3caa8236139f74f5a69641777b5b52d
      "Tokenize string.... DONE\n",
      "Remove non ascii... DONE\n",
      "Replace numbers.... DONE\n",
      "['are', 'dont', 'end', 'few', 'five', 'four', 'heigh', 'here', 'hi', 'high', 'highschool', 'i', 'one', 'school', 'six', 'socialsecuritynumber', 'test', 'that', 'think', 'this', 'three', 'two', 'use', 'what', 'will', 'words']\n"
     ]
    }
   ],
   "source": [
    "some_test_text = '''Here are a few words that I will use to test.\n",
    "\n",
    "heigh\n",
    "\n",
    "Don't think this is the end of it.\n",
    "\n",
    "high\n",
    "\n",
    "school\n",
    "\n",
    "Social Security Number\n",
    "\n",
    "ssn\n",
    "\n",
    "SSN\n",
    "\n",
    "high school\n",
    "\n",
    "social security number\n",
    "\n",
    "1. Hi hi hi\n",
    "2. Three four five\n",
    "6. What?'''\n",
    "\n",
    "print(sorted(set(normalize(some_test_text))))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 84,
=======
   "execution_count": 78,
>>>>>>> ee4d413fa3caa8236139f74f5a69641777b5b52d
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_count(words):\n",
    "    index_list = sorted(set(words))\n",
    "    count_list = list(range(len(index_list)))\n",
    "    for i in range(len(index_list)):\n",
    "        count_list[i] = 0\n",
    "        for word in words:\n",
    "            if word == index_list[i]:\n",
    "                count_list[i] = count_list[i] + 1\n",
    "    \n",
    "    grand_list = []\n",
    "    for i in range(len(index_list)):\n",
    "        item_list = []\n",
    "        item_list.append(count_list[i])\n",
    "        item_list.append(index_list[i])\n",
    "        grand_list.append(item_list)\n",
    "    \n",
    "    return sorted(grand_list, key=itemgetter(0), reverse=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 85,
=======
   "execution_count": 79,
>>>>>>> ee4d413fa3caa8236139f74f5a69641777b5b52d
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove punctuation. DONE\n",
      "Converting case.... DONE\n",
      "Remove freq words.. DONE\n",
<<<<<<< HEAD
      "Handle compounds... DONE\n",
=======
      "Handle compounds.. DONE\n",
>>>>>>> ee4d413fa3caa8236139f74f5a69641777b5b52d
      "Tokenize string.... DONE\n",
      "Remove non ascii... DONE\n",
      "Replace numbers.... DONE\n"
     ]
    }
   ],
   "source": [
    "sorted_list = get_sorted_count(normalize(some_test_text))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 86,
=======
   "execution_count": 80,
>>>>>>> ee4d413fa3caa8236139f74f5a69641777b5b52d
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 4 occurances of : socialsecuritynumber\n",
      "There were 3 occurances of : hi\n",
      "There were 1 occurances of : are\n",
      "There were 1 occurances of : dont\n",
      "There were 1 occurances of : end\n",
      "There were 1 occurances of : few\n",
      "There were 1 occurances of : five\n",
      "There were 1 occurances of : four\n",
      "There were 1 occurances of : heigh\n",
      "There were 1 occurances of : here\n",
      "There were 1 occurances of : high\n",
      "There were 1 occurances of : highschool\n",
      "There were 1 occurances of : i\n",
      "There were 1 occurances of : one\n",
      "There were 1 occurances of : school\n",
      "There were 1 occurances of : six\n",
      "There were 1 occurances of : test\n",
      "There were 1 occurances of : that\n",
      "There were 1 occurances of : think\n",
      "There were 1 occurances of : this\n",
      "There were 1 occurances of : three\n",
      "There were 1 occurances of : two\n",
      "There were 1 occurances of : use\n",
      "There were 1 occurances of : what\n",
      "There were 1 occurances of : will\n",
      "There were 1 occurances of : words\n"
     ]
    }
   ],
   "source": [
    "for word in sorted_list:\n",
    "    print('There were {} occurances of : {}'.format(word[0], word[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
