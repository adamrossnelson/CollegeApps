{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape college applications\n",
    "\n",
    "This notebook was developed to work with pdfs scraped with `ColAppScrape.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import os, sys\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from operator import itemgetter\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def stem_words(words):\n",
    "    # Stem words in list of tokenized words\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    # Lemmatize verbs in list of tokenized words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def remove_pmarks(text):\n",
    "    text = re.sub(r'~|`|:|;|\"|,|=|-|_|#', '', text)\n",
    "    text = str.replace(text, '\"', '')\n",
    "    text = str.replace(text, \"'\", '')\n",
    "    text = str.replace(text, '.', '')\n",
    "    text = str.replace(text, '?', '')\n",
    "    text = str.replace(text, ')', '')\n",
    "    text = str.replace(text, '(', '')\n",
    "    text = str.replace(text, '|', '')\n",
    "    text = str.replace(text, '$', '')\n",
    "    text = str.replace(text, '%', '')\n",
    "    return text\n",
    "\n",
    "def remove_nontxt(text):\n",
    "    text = str.replace(text, r'\\\\n', '')\n",
    "    text = str.replace(text, r'\\n', '')\n",
    "    return(text)\n",
    "\n",
    "def remove_common_words(text):\n",
    "    words_to_drop = ['a', 'all', 'an', 'and', 'are', 'as', 'at', 'be', 'but',\n",
    "                     'by', 'can', 'for', 'i', 'if', 'in', 'is', 'it', \n",
    "                     'my', 'not', 'of', 'on', 'or', 'that', \n",
    "                     'the', 'to', 'will', 'with', 'you']\n",
    "    for word in words_to_drop:\n",
    "        text = re.sub(''.join((r'\\b', word, r'\\b')), '', text)\n",
    "    return text\n",
    "\n",
    "def combine_cword(text):\n",
    "    compound_words = {\n",
    "        'social security number': 'socialsecuritynumber',\n",
    "        'ssn': 'socialsecuritynumber',\n",
    "        'high school': 'highschool'\n",
    "    }\n",
    "    for cword in compound_words:\n",
    "        text = re.sub(''.join((r'\\b', cword, r'\\b')), compound_words[cword], text)\n",
    "    return text\n",
    "\n",
    "def get_words(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    # Remove non-ASCII characters from list of tokenized words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    # Replace all interger occurrences in list of tokenized words with textual representation\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    # Remove all intergers\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit() == False:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def replace_nolen(words):\n",
    "    # Replcae words that are no length (i.e. '')\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if len(word) > 0:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(text):\n",
    "    # Remove punctuation from entire string.\n",
    "    # print('Remove punctuation.', end='')\n",
    "    text = remove_pmarks(text)\n",
    "    # print(' DONE')\n",
    "    # Remove non text\n",
    "    # print('Remove non text....', end='')\n",
    "    text = remove_nontxt(text)\n",
    "    # print(' DONE')\n",
    "    # Put to lowercase first\n",
    "    # print('Converting case....', end='')\n",
    "    text = text.lower()\n",
    "    # print(' DONE')\n",
    "    # Remove unwanted common words.\n",
    "    # print('Remove freq words..', end='')\n",
    "    text = remove_common_words(text)\n",
    "    # print(' DONE')\n",
    "    # Handle common application compound words\n",
    "    # print('Handle compounds...', end='')\n",
    "    text = combine_cword(text)\n",
    "    # print(' DONE')\n",
    "    # Tokenize the string.\n",
    "    # print('Tokenize string....', end='')\n",
    "    words = get_words(text)\n",
    "    # print(' DONE')\n",
    "    # Remove ascii characters.\n",
    "    # print('Remove non ascii...', end='')\n",
    "    words = remove_non_ascii(words)\n",
    "    # print(' DONE')\n",
    "    # Convert numebrs to words.\n",
    "    # print('Replace numbers....', end='')\n",
    "    # words = replace_numbers(words)\n",
    "    # print(' DONE')\n",
    "    # Convert numebrs to words.\n",
    "    # print('Remove numbers.....', end='')\n",
    "    words = remove_numbers(words)\n",
    "    # print(' DONE')\n",
    "    # Remove zero length words.\n",
    "    # print('Remove zero length.', end='')\n",
    "    words = replace_nolen(words)\n",
    "    # print(' DONE')\n",
    "    return words\n",
    "\n",
    "def get_sorted_count(words):\n",
    "    index_list = sorted(set(words))\n",
    "    count_list = list(range(len(index_list)))\n",
    "    for i in range(len(index_list)):\n",
    "        count_list[i] = 0\n",
    "        for word in words:\n",
    "            if word == index_list[i]:\n",
    "                count_list[i] = count_list[i] + 1\n",
    "    grand_list = []\n",
    "    for i in range(len(index_list)):\n",
    "        item_list = []\n",
    "        item_list.append(index_list[i])\n",
    "        item_list.append(count_list[i])\n",
    "        grand_list.append(item_list)\n",
    "    return sorted(grand_list, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cat', 1], ['dog', 1], ['hihi', 1]]\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['cat', 1], ['dog', 1], ['hihi', 1], ['new', 2], ['new', 2]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlist = ['hihi','cat','dog']\n",
    "holder = get_sorted_count(testlist)\n",
    "print(holder)\n",
    "print(holder.append(['new',2]))\n",
    "\n",
    "holder.append(['new',2])\n",
    "holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlist = ['hihi','cat','dog']\n",
    "holder = get_sorted_count(testlist).append(['new',2])\n",
    "holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_apps_df = DataFrame([], columns=['word','fq0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = open(os.path.join('pprapps', 'carver.edu2.pdf'), 'rb')\n",
    "test_pdf = PyPDF2.PdfFileReader(test_file)\n",
    "test_pdf.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49861b295f464269b48a0cbc7017038d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aacc.edu0.pdf\n",
      "a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# PyPDF2 has an undocumented PdfReadWarning\n",
    "# (PdfReadWarning: Superfluous whitespace found in object header b'1' b'0')\n",
    "# Example: Throws this warning for carver.edu2.pdf (among others)\n",
    "# https://stackoverflow.com/questions/5644836/in-python-how-does-one-catch-warnings-as-if-they-were-exceptions/39077786\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"error\")\n",
    "\n",
    "error_log = []\n",
    "\n",
    "# for file in tqdm_notebook(os.listdir(os.path.join('pprapps'))[224:228]):\n",
    "# for file in os.listdir(os.path.join('pprapps'))[:3]:\n",
    "for file in tqdm_notebook(os.listdir(os.path.join('pprapps'))[0:1]):\n",
    "    print(file)\n",
    "    try:\n",
    "        print('a')\n",
    "        app_file = PyPDF2.PdfFileReader(\n",
    "            open(os.path.join('pprapps', file), 'rb'))\n",
    "    except:\n",
    "        print('b')\n",
    "        error_log.append('Error loading : ' + os.path.join(\n",
    "            'pprapps', file))\n",
    "    \n",
    "    # num_of_pgs = app_file.numPages\n",
    "    try:\n",
    "        print('c')\n",
    "        num_of_pgs = app_file.numPages\n",
    "    except:\n",
    "        print('d')\n",
    "        num_of_pgs = 100\n",
    "        error_log.append('Error getting no of pgs : ' + os.path.join(\n",
    "            'pprapps', file))\n",
    "        \n",
    "    if num_of_pgs < 20:\n",
    "        print('e')\n",
    "        app_file_text = []\n",
    "        for pageNum in range(app_file.numPages):\n",
    "            app_file_text.append(\n",
    "                app_file.getPage(pageNum).extractText().encode(\n",
    "                    'utf-8', 'ignore').decode('utf-8', 'ignore'))\n",
    "        # Use file name to create an unique header\n",
    "        f_col = str.replace(file, 'pdf', '')\n",
    "        f_col = f_col[:-1]\n",
    "        \n",
    "        app_file_listed = get_sorted_count(\n",
    "            normalize(str(app_file_text)))\n",
    "        app_file_listed.append(['aaaaaaa',num_of_pgs])\n",
    "        \n",
    "        next_record = DataFrame(app_file_listed, columns=['word',f_col])\n",
    "        \n",
    "        #next_record = DataFrame(get_sorted_count(\n",
    "        #    normalize(str(app_file_text))), columns=['word',f_col])\n",
    "        \n",
    "        # next_record.append(['0pages',num_of_pgs])\n",
    "        \n",
    "        # print(next_record)\n",
    "        \n",
    "        # vars()[str.replace(file, '.', '')] = DataFrame(\n",
    "        #    get_sorted_count(normalize(str(app_file_text))), \n",
    "        #            columns=['word',f_col])\n",
    "        \n",
    "        #all_apps_df = pd.merge(\n",
    "        #    all_apps_df, vars()[str.replace(\n",
    "        #        file, '.', '')], on='word', how='outer')\n",
    "        \n",
    "        all_apps_df = pd.merge(\n",
    "            all_apps_df, next_record, on='word', how='outer')\n",
    "    else:\n",
    "        print('f')\n",
    "        error_log.append('Longer than 19 pgs : ' + os.path.join(\n",
    "            'pprapps', file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fq0</th>\n",
       "      <th>word</th>\n",
       "      <th>aacc.edu0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>0</td>\n",
       "      <td>aaaaaaa</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fq0     word  aacc.edu0\n",
       "1152    0  aaaaaaa         12"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_apps_df.fillna(0)[all_apps_df['word'] == 'aaaaaaa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_isApp_df = pd.read_csv(\n",
    "    os.path.join('app_rec_train', 'list_of_isApp.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for app in list_of_isApp_df['rootdom']:\n",
    "    try:\n",
    "        # print(os.path.join('pprapps', app + '.pdf'))\n",
    "        app_c = PyPDF2.PdfFileReader(os.path.join('pprapps', app + '.pdf'))\n",
    "        lengths.append(app_c.numPages)\n",
    "        # lengths.append(app)\n",
    "    except FileNotFoundError:\n",
    "        print('{} : {}'.format(\n",
    "                'Could not find', os.path.join('pprapps', app + '.pdf')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_isApp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
