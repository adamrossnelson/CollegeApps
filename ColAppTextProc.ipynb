{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape college applications\n",
    "\n",
    "This notebook was developed to work with pdfs scraped with `ColAppScrape.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import os, sys\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from operator import itemgetter\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def stem_words(words):\n",
    "    # Stem words in list of tokenized words\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    # Lemmatize verbs in list of tokenized words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def remove_pmarks(text):\n",
    "    text = re.sub(r'~|`|:|;|\"|,|=|-|_|#', '', text)\n",
    "    text = str.replace(text, '\"', '')\n",
    "    text = str.replace(text, \"'\", '')\n",
    "    text = str.replace(text, '.', '')\n",
    "    text = str.replace(text, '?', '')\n",
    "    text = str.replace(text, ')', '')\n",
    "    text = str.replace(text, '(', '')\n",
    "    text = str.replace(text, '|', '')\n",
    "    text = str.replace(text, '$', '')\n",
    "    text = str.replace(text, '%', '')\n",
    "    return text\n",
    "\n",
    "def remove_nontxt(text):\n",
    "    text = str.replace(text, r'\\\\n', '')\n",
    "    text = str.replace(text, r'\\n', '')\n",
    "    return(text)\n",
    "\n",
    "def remove_common_words(text):\n",
    "    words_to_drop = ['a', 'all', 'an', 'and', 'are', 'as', 'at', 'be', 'but',\n",
    "                     'by', 'can', 'for', 'i', 'if', 'in', 'is', 'it', \n",
    "                     'my', 'not', 'of', 'on', 'or', 'that', \n",
    "                     'the', 'to', 'will', 'with', 'you']\n",
    "    for word in words_to_drop:\n",
    "        text = re.sub(''.join((r'\\b', word, r'\\b')), '', text)\n",
    "    return text\n",
    "\n",
    "def combine_cword(text):\n",
    "    compound_words = {\n",
    "        'social security number': 'socialsecuritynumber',\n",
    "        'ssn': 'socialsecuritynumber',\n",
    "        'high school': 'highschool'\n",
    "    }\n",
    "    for cword in compound_words:\n",
    "        text = re.sub(''.join((r'\\b', cword, r'\\b')), compound_words[cword], text)\n",
    "    return text\n",
    "\n",
    "def get_words(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    # Remove non-ASCII characters from list of tokenized words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    # Replace all interger occurrences in list of tokenized words with textual representation\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    # Remove all intergers\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit() == False:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def replace_nolen(words):\n",
    "    # Replcae words that are no length (i.e. '')\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if len(word) > 0:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(text):\n",
    "    # Remove punctuation from entire string.\n",
    "    # print('Remove punctuation.', end='')\n",
    "    text = remove_pmarks(text)\n",
    "    # print(' DONE')\n",
    "    # Remove non text\n",
    "    # print('Remove non text....', end='')\n",
    "    text = remove_nontxt(text)\n",
    "    # print(' DONE')\n",
    "    # Put to lowercase first\n",
    "    # print('Converting case....', end='')\n",
    "    text = text.lower()\n",
    "    # print(' DONE')\n",
    "    # Remove unwanted common words.\n",
    "    # print('Remove freq words..', end='')\n",
    "    text = remove_common_words(text)\n",
    "    # print(' DONE')\n",
    "    # Handle common application compound words\n",
    "    # print('Handle compounds...', end='')\n",
    "    text = combine_cword(text)\n",
    "    # print(' DONE')\n",
    "    # Tokenize the string.\n",
    "    # print('Tokenize string....', end='')\n",
    "    words = get_words(text)\n",
    "    # print(' DONE')\n",
    "    # Remove ascii characters.\n",
    "    # print('Remove non ascii...', end='')\n",
    "    words = remove_non_ascii(words)\n",
    "    # print(' DONE')\n",
    "    # Convert numebrs to words.\n",
    "    # print('Replace numbers....', end='')\n",
    "    # words = replace_numbers(words)\n",
    "    # print(' DONE')\n",
    "    # Convert numebrs to words.\n",
    "    # print('Remove numbers.....', end='')\n",
    "    words = remove_numbers(words)\n",
    "    # print(' DONE')\n",
    "    # Remove zero length words.\n",
    "    # print('Remove zero length.', end='')\n",
    "    words = replace_nolen(words)\n",
    "    # print(' DONE')\n",
    "    return words\n",
    "\n",
    "def get_sorted_count(words):\n",
    "    index_list = sorted(set(words))\n",
    "    count_list = list(range(len(index_list)))\n",
    "    for i in range(len(index_list)):\n",
    "        count_list[i] = 0\n",
    "        for word in words:\n",
    "            if word == index_list[i]:\n",
    "                count_list[i] = count_list[i] + 1\n",
    "    grand_list = []\n",
    "    for i in range(len(index_list)):\n",
    "        item_list = []\n",
    "        item_list.append(index_list[i])\n",
    "        item_list.append(count_list[i])\n",
    "        grand_list.append(item_list)\n",
    "    return sorted(grand_list, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_apps_df = DataFrame([], columns=['word','fq0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper = 2\n",
    "bar_len = len(os.listdir(os.path.join('pprapps')))\n",
    "for file in os.listdir(os.path.join('pprapps')):\n",
    "    print(file, ' ', str(stopper), ' ', str.replace(file, '.', ''))\n",
    "    app_file = PyPDF2.PdfFileReader(open(os.path.join('pprapps', file), 'rb'))\n",
    "    app_file_text = []\n",
    "    for pageNum in range(app_file.numPages):\n",
    "        app_file_text.append(app_file.getPage(pageNum).extractText().encode('utf-8', 'ignore').decode('utf-8', 'ignore'))\n",
    "    f_col = str.replace(file + 'fq', '.', '')\n",
    "    f_col = str.replace(f_col, 'edu', '')\n",
    "    f_col = str.replace(f_col, 'pdf', '')\n",
    "    vars()[str.replace(file, '.', '')] = DataFrame(get_sorted_count(normalize(str(app_file_text))), \n",
    "                columns=['word',f_col])\n",
    "    all_apps_df = pd.merge(all_apps_df, vars()[str.replace(file, '.', '')], on='word', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_apps_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
