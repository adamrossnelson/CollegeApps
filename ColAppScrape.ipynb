{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape college applications\n",
    "\n",
    "This notebook was developed following the `ColEarlyDecScrape.ipynb` prototype.\n",
    "\n",
    "## Step One: Prepare Directory Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load directory data.\n",
    "IPEDSfile = pd.read_stata('../../statadata/IPEDSDirInfo02to16smlr.dta', preserve_dtypes=False)\n",
    "\n",
    "# Keep 2 and 4yr institutions.\n",
    "IPEDSfile['filter'] = np.where((IPEDSfile['sector']=='Public, 4-year or above') | \n",
    "                               (IPEDSfile['sector']=='Public, 2-year') |\n",
    "                               (IPEDSfile['sector']=='Private not-for-profit, 4-year or above') |\n",
    "                               (IPEDSfile['sector']=='Private not-for-profit, 2-year'), 1, 0)\n",
    "IPEDSfile = IPEDSfile[IPEDSfile['filter']==1]\n",
    "\n",
    "# Remove www. prefix from webaddress.\n",
    "IPEDSfile['rootdom'] = IPEDSfile.loc[:, 'webaddr'].replace(regex=True, to_replace='www.', value='')\n",
    "\n",
    "# Remove miscellaneous slashes & other from webaddress.\n",
    "IPEDSfile['rootdom'] = IPEDSfile.loc[:, 'rootdom'].replace(regex=True, to_replace=r'/', value='')\n",
    "IPEDSfile['rootdom'] = IPEDSfile.loc[:, 'rootdom'].replace(regex=True, to_replace=r'HTTPS:', value='')\n",
    "IPEDSfile['rootdom'] = IPEDSfile.loc[:, 'rootdom'].replace(regex=True, to_replace=r'https:', value='')\n",
    "IPEDSfile['rootdom'] = IPEDSfile.loc[:, 'rootdom'].replace(regex=True, to_replace=r'about', value='')\n",
    "\n",
    "# Reset the data frame's index.\n",
    "IPEDSfile = IPEDSfile.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell copes with unanticipated errors and/or connection time-out problems.\n",
    "# Keep record of last unsuccessful iteration here:\n",
    "#    Encountered CAPTCHA at stillman.edu               April 17, 2018\n",
    "#    Encountered NaN after alaskapacific.edu           April 18, 2018\n",
    "#    Encountered CAPTCHA at buc.edu                    April 18, 2018\n",
    "#    Program STALLED at arapahoe.edu                   April 18, 2018\n",
    "#    Program STALLED at howard.edu                     April 19, 2018\n",
    "#    Took a break from project at babson.edu           April 19, 2018\n",
    "#    Took a break from project at marrimack.edu        April 25, 2018\n",
    "\n",
    "# Get user input regarding previous scrape attempts / error log above.\n",
    "print('Enter the school starting school root domain name. No entry will start at beginning of the list.')\n",
    "start_school = input()\n",
    "\n",
    "# If user provided starting school, remove preceeding schools.\n",
    "if start_school != '':\n",
    "    new_school_loc = IPEDSfile[IPEDSfile['rootdom'] == start_school].index.tolist()[0]\n",
    "    IPEDSfile = IPEDSfile[new_school_loc:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of domains to scrape & check results.\n",
    "weblist = IPEDSfile['rootdom']\n",
    "weblist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell writes weblist to a csv that can be used in App_Rec_Train/aaa_appcodes.do\n",
    "with open('aaa_appcodestarter' + '.csv', mode='w') as csvfile:\n",
    "            print('webindx', file = csvfile)\n",
    "            for csv_lines in weblist:\n",
    "                print(csv_lines, file = csvfile)\n",
    "csvfile.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Two: Prepare Browser Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Short list of url domains set aside for testing purposes.\n",
    "# weblist = ['babson.edu', 'boston.edu', 'uwec.edu', 'wisc.edu']\n",
    "# weblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser = webdriver.Chrome()\n",
    "browser = webdriver.Firefox()\n",
    "print('Loaded Browser Here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Three: Scrape For Each School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to store record of errors.\n",
    "log_list = []\n",
    "\n",
    "# Iteratively loop through each institution's website.\n",
    "for school in weblist:\n",
    "    print('STARTING SCHOOL', str(school).upper())\n",
    "    log_list.append('STARTING SCHOOL ' + str(school).upper())\n",
    "    searchstr = ''.join((r'https://www.google.com/search?q=',\n",
    "                         r'application+admission+AND+(printable+OR+paper+OR+mail)+site:', \n",
    "                         school, r'+filetype:pdf'))\n",
    "    log_list.append('Search string is ' + searchstr)\n",
    "    browser.get(searchstr)\n",
    "    results = browser.find_elements_by_css_selector('h3 > a')\n",
    "\n",
    "    # Test if there were results.\n",
    "    if len(results) == 0:\n",
    "        try:\n",
    "            # Occassionally Google responds with CAPTCHA verification challenge.\n",
    "            # If Google responds with CAPTCHA routine will wait for user to complete CAPTCHA challenge.\n",
    "            check_captcha = browser.find_element_by_partial_link_text('Why did this happen')\n",
    "            print('At school, ', school.lower(), 'Google responded with CAPTCHA - Will wait for user input.', end='\\n\\n')\n",
    "            log_list.append(school + '-' + '.' + '-EncounteredCAPTCHA-' + '.')\n",
    "            discarded_wait = input()\n",
    "            results = browser.find_elements_by_css_selector('h3 > a')\n",
    "        except:\n",
    "            # Occasionally Google responds with no results.\n",
    "            print('No results for school ', school.lower())\n",
    "            log_list.append(school + '-' + '.' + '-NoResults-' + '.')\n",
    "            sleep(.5)\n",
    "\n",
    "    # Test the number of results. Download up to the first three results.\n",
    "    if len(results) < 3:\n",
    "        doc_count = len(results)\n",
    "        log_list.append('There were {} results from Google. Will download all.'.format(len(results)))\n",
    "        sleep(.1)\n",
    "    else:\n",
    "        doc_count = 3\n",
    "        log_list.append('There were 10 or more results from Google. Will download first three.')\n",
    "        sleep(.2)\n",
    "\n",
    "    # From above if len(results) == 0 then doc_count will also be zero which will skip this loop.\n",
    "    for i in range(doc_count):\n",
    "        filelink = results[i].get_attribute('href')\n",
    "        try:\n",
    "            pdf = requests.get(filelink)\n",
    "            fname = ''.join((school, str(i), '.pdf'))\n",
    "            open(os.path.join('pprapps', fname), 'wb').write(pdf.content)\n",
    "            log_list.append('Filelink = ' + filelink)\n",
    "            log_list.append('Saved as : ' + fname)\n",
    "            log_list.append('Header info :' + str(pdf.headers))\n",
    "        except ConnectionError:\n",
    "            print('There was a ConnectionError on the {}th iteration at : {}'.format(str(i), school.lower()))\n",
    "            log_list.append(school + '-' + str(i) + '-ConnectionError-' + filelink)\n",
    "            sleep(.1)\n",
    "        except:\n",
    "            print('There was an UnspecifiedError on the {}th iteration at : {}'.format(str(i), school))\n",
    "            log_list.append(school + '-' + str(i) + '-UnspecifiedError-' + filelink)\n",
    "            sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the error log to a file for later reference.\n",
    "with open('log_' + str(datetime.datetime.now())[2:16].replace(\" \", \"-\").replace(\":\",\"\") + '.log',\n",
    "          mode='w') as logfile:\n",
    "            print('This is the error log file from {}'.format(str(datetime.datetime.now())), file = logfile)\n",
    "            for log_lines in log_list:\n",
    "                print(log_lines, file = logfile)\n",
    "logfile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
